# -*- coding: utf-8 -*-
"""Churn_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16UHhv5U9ihYJho2SwkxfZxg4VaYeVH4-
"""

# STEP 1: SETUP & DATA LOADING

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Load the dataset
# Ensure the filename matches exactly what you uploaded
df = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')

# Display first 5 rows
df.head()

# STEP 2: DATA CLEANING

# 1. Convert 'TotalCharges' to numeric (forcing errors to NaN)
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

# 2. Drop rows with missing values (11 rows found)
df.dropna(inplace=True)

# 3. Drop 'customerID' (It is unique ID and has no predictive power)
df.drop(columns=['customerID'], inplace=True)

# 4. Convert Target 'Churn' to binary (1 for Yes, 0 for No)
df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})

# Verify the changes
print("Data Cleaned Successfully!")
df.info()

# STEP 3: VISUALIZATION

# Set the style
sns.set(style="whitegrid")

# Create a figure with subplots for a clean layout
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Plot 1: Churn Distribution
sns.countplot(x='Churn', data=df, ax=axes[0], palette='pastel')
axes[0].set_title('Overall Churn Distribution')

# Plot 2: Contract Type vs Churn
sns.countplot(x='Contract', hue='Churn', data=df, ax=axes[1], palette='Set2')
axes[1].set_title('Churn by Contract Type')

# Plot 3: Tenure Distribution
sns.histplot(x='tenure', hue='Churn', data=df, multiple='stack', ax=axes[2], palette='Set1')
axes[2].set_title('Churn by Tenure (Months)')

plt.tight_layout()
plt.show()

# STEP 4: PREPROCESSING

# 1. One-Hot Encoding (Convert text columns to numbers)
df_dummy = pd.get_dummies(df, drop_first=True)

# 2. Define Features (X) and Target (y)
X = df_dummy.drop('Churn', axis=1)
y = df_dummy['Churn']

# 3. Split the data (80% Train, 20% Test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Check the results
print(f"Original Features: {df.shape[1]}")
print(f"Encoded Features:  {df_dummy.shape[1]}")
print(f"Training Samples:  {X_train.shape[0]}")
print(f"Testing Samples:   {X_test.shape[0]}")

# STEP 5: MODEL TRAINING

# 1. Train the Model
# We increase max_iter to 5000 to ensure the math converges
model = LogisticRegression(max_iter=5000)
model.fit(X_train, y_train)

# 2. Make Predictions
y_pred = model.predict(X_test)

# 3. Evaluate
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy:.2f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# 4. Confusion Matrix Visualization
plt.figure(figsize=(6, 4))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.ylabel('Actual Label')
plt.xlabel('Predicted Label')
plt.show()

# STEP 6: FEATURE IMPORTANCE

# 1. Get Model Coefficients
weights = pd.Series(model.coef_[0], index=X.columns.values)
weights = weights.sort_values(ascending=False)

# 2. Visualize Top Positive and Negative Factors
plt.figure(figsize=(10, 6))

# Combine top 5 churn drivers (positive) and top 5 retention drivers (negative)
top_weights = pd.concat([weights.head(5), weights.tail(5)])

# Create bar plot
colors = ['red' if x < 0 else 'green' for x in top_weights]
top_weights.plot(kind='bar', color=colors)

plt.title('Top Factors Driving Churn (Green) vs. Retention (Red)')
plt.ylabel('Impact Score (Coefficient)')
plt.axhline(0, color='black', linewidth=0.8)
plt.show()

# Print specific insights
print("Top Churn Driver:", weights.idxmax())
print("Top Retention Driver:", weights.idxmin())